User
Nand2tetris第十章內容和怎麼實作

Model
ThinkingThoughts
Expand to view model thoughts

chevron_right
Nand2Tetris 第十章 (Chapter 10) 的主題是「編譯器 I：語法分析 (Compiler I: Syntax Analysis)」。
這一章是從「軟體層次」邁向「編譯器」的第一步。你的目標是將高階語言（Jack 語言）的原始碼，轉換成一種結構化的格式（在這個專案中是 XML 格式的解析樹）。這章不涉及程式碼生成（那是在第 11 章），只專注於讀懂程式碼。
以下我將分為核心內容與實作指南兩部分來詳細說明。
第一部分：核心內容 (理論基礎)
這一章的工作可以分為兩個主要階段：
1. 詞法分析 (Lexical Analysis / Tokenizing)
編譯器看不懂整行的字串（如 while (count > 0)），它必須先將其拆解成有意義的最小單位，稱為 Token。
Jack 語言定義了五種 Token：
Keyword (關鍵字): class, constructor, function, method, field, static, var, int, char, boolean, void, true, false, null, this, let, do, if, else, while, return.
Symbol (符號): {, }, (, ), [, ], ., ,, ;, +, -, *, /, &, |, <, >, =, ~.
IntegerConstant (整數常數): 0...32767.
StringConstant (字串常數): 被雙引號包圍的字元序列。
Identifier (識別字): 變數名、類別名、函數名（非關鍵字的文字）。
2. 語法分析 (Syntax Analysis / Parsing)
將上述的一串 Token，根據 Jack 語言的文法規則 (Grammar)，組合成樹狀結構。
我們使用的方法是 遞歸下降解析 (Recursive Descent Parsing)。這意味著對於文法中的每一個「非終結符號 (Non-terminal)」，你都要寫一個對應的函數來處理它。
目標輸出：
一個 XML 檔案，清楚地標示出程式的結構。例如：
code
Xml
<class>
  <keyword> class </keyword>
  <identifier> Main </identifier>
  <symbol> { </symbol>
  ...
</class>
第二部分：實作指南 (如何寫程式)
你需要編寫三個模組（可以使用 Python, Java, C++ 等你熟悉的語言）：
1. JackTokenizer (分詞器)
這是最底層的模組，負責讀檔並吐出 Token。
功能需求：
去除原始碼中的註釋 (//, /* ... */) 和多餘空白。
能夠一個接一個地回傳 Token。
能夠識別當前 Token 的類型（是關鍵字？還是數字？）。
實作技巧：
正則表達式 (Regex) 是好朋友。你可以寫一個大的 Regex 來匹配各種類型的 Token。
或者使用簡單的狀態機：逐字元讀取，遇到空格或符號就切斷。
特殊符號處理：在輸出 XML 時，<, >, &, " 必須轉義為 &lt;, &gt;, &amp;, &quot;。
2. CompilationEngine (編譯引擎)
這是核心邏輯所在。它會從 Tokenizer 拿 Token，並輸出 XML。
你需要為 Jack 文法中的每個結構寫一個方法。根據書中提供的 Grammar 規範：
程式結構類方法：
compileClass(): 處理 class Name { ... }
compileClassVarDec(): 處理 static int x;
compileSubroutine(): 處理 function, method, constructor
compileParameterList(): 處理 (int x, int y)
compileSubroutineBody(): 處理 { var ... statements }
compileVarDec(): 處理 var int i;
語句類方法 (Statements)：
compileStatements(): 是一個總控，根據下一個 Token 是 let, if, while 等呼叫對應方法。
compileLet(), compileIf(), compileWhile(), compileDo(), compileReturn()。
表達式類方法 (Expressions) - 最難的部分：
compileExpression(): 處理 x + 5 或 a[i] * foo()。
compileTerm(): 處理表達式中的單元（整數、變數、括號表達式、函數呼叫）。
compileExpressionList(): 處理函數呼叫參數列表 (a, b, c)。
3. JackAnalyzer (主程式)
負責接收輸入檔案（或是目錄）。
如果是目錄，就對裡面所有的 .jack 檔進行處理。
建立 JackTokenizer 和 CompilationEngine 的實例並串接起來。
實作步驟與常見陷阱 (Walkthrough)
建議依照以下順序實作，以通過 Course Grader 的測試：
步驟 1：先搞定 Tokenizer
不要急著寫 Parser。先寫好 Tokenizer，並確保它能產出與 MainT.xml (T 代表 Tokens) 一模一樣的輸出。
難點：正確移除區塊註釋 /* ... */，特別是當註釋跨越多行或是出現在程式碼中間時。
步驟 2：搭建 CompilationEngine 骨架
建立所有 compileXXX 方法，但內容先留空或只印出簡單的標籤。
步驟 3：實作「結構」與「變數宣告」
先實作 compileClass, compileClassVarDec, compileVarDec。
邏輯：例如 compileVarDec，邏輯是：
輸出 <varDec>
預期吃掉一個 var (keyword)
預期吃掉一個 Type (int/char/boolean/ClassName)
預期吃掉一個變數名
如果下一個是 ,，重複讀取變數名（例如 var int x, y, z;）
預期吃掉 ;
輸出 </varDec>
步驟 4：實作語句 (Statements)
Let, Do, If, While, Return 相對簡單，因為它們的結構很固定。
注意：compileStatements 處理的是一系列語句，直到遇到 } 為止。
步驟 5：實作表達式 (Expressions) —— 大魔王
這是第 10 章最容易卡關的地方。Jack 的表達式文法是：
expression: term (op term)*
這意味著你要先呼叫 compileTerm()，然後檢查下一個 Token 是不是運算符號 (+, -, *, / 等)。如果是，就輸出該符號，然後再次呼叫 compileTerm()。
compileTerm 的 Lookahead (向前看) 問題：
當你讀到一個 Identifier (例如 foo) 時，你不知道它是單純的變數、陣列、還是函數呼叫。你需要偷看下一個 Token：
如果是 [，它是陣列 (foo[i])。
如果是 (，它是函數呼叫 (foo())。
如果是 .，它是方法呼叫 (obj.method())。
如果都不是，它就是單純變數。
步驟 6：使用 TextComparer 進行測試
Nand2Tetris 提供的 TextComparer 工具非常嚴格。
空白敏感：XML 的縮排（Indentation）必須完全符合預期（通常是每一層級縮排 2 個空格）。
順序：Token 的順序不能錯。
總結實作心法
遞歸思維：相信你的函數。當你需要解析一個 expression 時，直接呼叫 compileExpression()，不要去想它內部細節，假設它會正確完成工作。
Eat / Expect 函數：寫一個輔助函數（例如 process(expectedToken)）。如果當前的 Token 符合預期，就輸出 XML 並讀取下一個；如果不符合，就報錯。這會讓你的程式碼乾淨很多。
由簡入繁：
先過 ExpressionLessSquare (沒有複雜表達式的程式)。
再過 Square (標準程式)。
最後過 ArrayTest (包含陣列與複雜運算)。
第 10 章完成後，你得到的是一堆 XML，這看起來沒什麼用，但它是第 11 章（將 XML 轉成 VM Code）的絕對地基。語法分析做對了，下一章的程式碼生成就會輕鬆很多。
Model
ThinkingThoughts
Expand to view model thoughts

chevron_right
這是一個非常好的練習。用 Python 實作 Nand2Tetris 第 10 章的 Compiler 是很多人的選擇，因為 Python 的字串處理非常方便。
以下是一個完整的、單一檔案的實作範例。為了讓你容易理解，我將 JackTokenizer 和 CompilationEngine 寫在同一個腳本中，並加上了詳細的中文註解。
程式碼結構說明
Lexical Definition: 定義關鍵字與符號。
JackTokenizer: 負責讀檔、去除註釋、利用 Regular Expression (正則表達式) 將程式碼切分成 Token。
CompilationEngine: 核心語法分析器，利用遞歸下降法 (Recursive Descent) 產生 XML。
Main: 讀取檔案並執行編譯。
Python 完整實作代碼
你可以將以下程式碼存為 JackAnalyzer.py，然後在終端機執行 python JackAnalyzer.py <你的檔案或是目錄路徑>。
code
Python
import sys
import os
import re

# ==========================================
# 1. 基礎定義 (關鍵字與符號)
# ==========================================

KEYWORDS = {
    'class', 'constructor', 'function', 'method', 'field', 'static', 'var',
    'int', 'char', 'boolean', 'void', 'true', 'false', 'null', 'this',
    'let', 'do', 'if', 'else', 'while', 'return'
}

SYMBOLS = {
    '{', '}', '(', ')', '[', ']', '.', ',', ';', '+', '-', '*', '/', '&',
    '|', '<', '>', '=', '~'
}

# XML 特殊字元轉義
XML_MAP = {
    '<': '&lt;',
    '>': '&gt;',
    '"': '&quot;',
    '&': '&amp;'
}

# 運算子集合 (用於 Expression 解析)
OPS = {'+', '-', '*', '/', '&', '|', '<', '>', '='}

# ==========================================
# 2. JackTokenizer (分詞器)
# ==========================================

class JackTokenizer:
    def __init__(self, input_file):
        with open(input_file, 'r') as f:
            content = f.read()
        
        self.tokens = self._tokenize(content)
        self.current_token_idx = 0
        self.current_token = ""
        self.token_type = ""

    def _tokenize(self, content):
        """
        移除註釋並使用 Regex 拆分 Token
        """
        # 1. 移除區塊註釋 /* ... */
        # re.S 讓 . 可以匹配換行符號
        content = re.sub(r'/\*.*?\*/', ' ', content, flags=re.S)
        
        # 2. 移除單行註釋 // ...
        content = re.sub(r'//.*', ' ', content)
        
        # 3. 定義 Token 的 Regex 模式
        # 順序很重要：先抓字串常數，再抓符號，再抓整數/關鍵字/識別字
        # Group 1: String Constant "..."
        # Group 2: Symbol
        # Group 3: Integer / Identifier / Keyword
        token_pattern = re.compile(r'(".*?"|[{}().,;\[\]+\-*/&|<>=~])|(\S+)')
        
        raw_tokens = []
        # split() 會自動去除多餘空白，但我們需要保留符號的獨立性
        # 這裡我們不直接 split，而是用 findall 或自製邏輯
        # 為了簡單起見，我們利用一個強大的 Regex 來提取所有 token
        
        # 更精確的 Regex：
        # keyword/id | int | string | symbol
        regex = r'([a-zA-Z_]\w*)|(\d+)|("[^"\n]*")|([{}()\[\].,;+\-*/&|<>=~])'
        
        matches = re.findall(regex, content)
        
        tokens = []
        for match in matches:
            # match 是一個 tuple，例如 ('class', '', '', '') 或 ('', '123', '', '')
            for group in match:
                if group:
                    tokens.append(group)
        return tokens

    def has_more_tokens(self):
        return self.current_token_idx < len(self.tokens)

    def advance(self):
        if self.has_more_tokens():
            self.current_token = self.tokens[self.current_token_idx]
            self.current_token_idx += 1
            self._set_token_type()

    def _set_token_type(self):
        token = self.current_token
        if token in KEYWORDS:
            self.token_type = 'KEYWORD'
        elif token in SYMBOLS:
            self.token_type = 'SYMBOL'
        elif token.isdigit():
            self.token_type = 'INT_CONST'
        elif token.startswith('"'):
            self.token_type = 'STRING_CONST'
        else:
            self.token_type = 'IDENTIFIER'

    # 回傳當前 token 的各種屬性
    def key_word(self): return self.current_token
    def symbol(self): return self.current_token
    def identifier(self): return self.current_token
    def int_val(self): return self.current_token
    def string_val(self): return self.current_token[1:-1] # 去除引號

    # Lookahead (偷看下一個 Token，這在 Parser 中很重要)
    def peek(self):
        if self.has_more_tokens():
            return self.tokens[self.current_token_idx]
        return ""

# ==========================================
# 3. CompilationEngine (語法分析引擎)
# ==========================================

class CompilationEngine:
    def __init__(self, tokenizer, output_file):
        self.tokenizer = tokenizer
        self.outfile = open(output_file, 'w')
        self.indent_level = 0
        
        # 初始化：先讀取第一個 Token
        if self.tokenizer.has_more_tokens():
            self.tokenizer.advance()

    def close(self):
        self.outfile.close()

    # --- 輔助方法 ---
    
    def _write_tag(self, tag, value=None, is_terminal=False):
        indent = '  ' * self.indent_level
        if is_terminal:
            # 處理 XML 特殊字元
            val_escaped = XML_MAP.get(value, value)
            self.outfile.write(f"{indent}<{tag}> {val_escaped} </{tag}>\n")
        else:
            # 非終結符號 (結構標籤)
            if value == 'start':
                self.outfile.write(f"{indent}<{tag}>\n")
                self.indent_level += 1
            elif value == 'end':
                self.indent_level -= 1
                indent = '  ' * self.indent_level # 更新縮排
                self.outfile.write(f"{indent}</{tag}>\n")

    def _process(self, expected_token=None):
        """
        處理並輸出當前的 Token (Terminal)，然後讀取下一個。
        如果指定了 expected_token，則會檢查是否匹配 (簡單的錯誤檢查)。
        """
        curr_token = self.tokenizer.current_token
        curr_type = self.tokenizer.token_type
        
        tag_map = {
            'KEYWORD': 'keyword',
            'SYMBOL': 'symbol',
            'IDENTIFIER': 'identifier',
            'INT_CONST': 'integerConstant',
            'STRING_CONST': 'stringConstant'
        }
        
        tag = tag_map.get(curr_type)
        
        if curr_type == 'STRING_CONST':
            value = self.tokenizer.string_val() # 去引號
        else:
            value = curr_token

        self._write_tag(tag, value, is_terminal=True)
        self.tokenizer.advance()

    # --- 結構編譯方法 (Structure) ---

    def compile_class(self):
        self._write_tag('class', 'start')
        
        self._process('class')      # 'class'
        self._process()             # ClassName
        self._process('{')          # '{'
        
        # 處理 ClassVarDec (static/field)
        while self.tokenizer.current_token in ['static', 'field']:
            self.compile_class_var_dec()
            
        # 處理 Subroutine (constructor/function/method)
        while self.tokenizer.current_token in ['constructor', 'function', 'method']:
            self.compile_subroutine()
            
        self._process('}')          # '}'
        self._write_tag('class', 'end')

    def compile_class_var_dec(self):
        self._write_tag('classVarDec', 'start')
        self._process() # static | field
        self._process() # type
        self._process() # varName
        
        # 處理多個變數宣告 (例如 static int x, y;)
        while self.tokenizer.current_token == ',':
            self._process(',')
            self._process() # varName
            
        self._process(';')
        self._write_tag('classVarDec', 'end')

    def compile_subroutine(self):
        self._write_tag('subroutineDec', 'start')
        self._process() # constructor | function | method
        self._process() # void | type
        self._process() # subroutineName
        self._process('(')
        self.compile_parameter_list()
        self._process(')')
        self.compile_subroutine_body()
        self._write_tag('subroutineDec', 'end')

    def compile_parameter_list(self):
        self._write_tag('parameterList', 'start')
        # 如果不是 ')'，表示有參數
        if self.tokenizer.current_token != ')':
            self._process() # type
            self._process() # varName
            while self.tokenizer.current_token == ',':
                self._process(',')
                self._process() # type
                self._process() # varName
        self._write_tag('parameterList', 'end')

    def compile_subroutine_body(self):
        self._write_tag('subroutineBody', 'start')
        self._process('{')
        
        # 變數宣告 (var)
        while self.tokenizer.current_token == 'var':
            self.compile_var_dec()
            
        # 語句 (Statements)
        self.compile_statements()
        
        self._process('}')
        self._write_tag('subroutineBody', 'end')

    def compile_var_dec(self):
        self._write_tag('varDec', 'start')
        self._process('var')
        self._process() # type
        self._process() # varName
        while self.tokenizer.current_token == ',':
            self._process(',')
            self._process() # varName
        self._process(';')
        self._write_tag('varDec', 'end')

    # --- 語句編譯方法 (Statements) ---

    def compile_statements(self):
        self._write_tag('statements', 'start')
        while self.tokenizer.current_token in ['let', 'if', 'while', 'do', 'return']:
            if self.tokenizer.current_token == 'let':
                self.compile_let()
            elif self.tokenizer.current_token == 'if':
                self.compile_if()
            elif self.tokenizer.current_token == 'while':
                self.compile_while()
            elif self.tokenizer.current_token == 'do':
                self.compile_do()
            elif self.tokenizer.current_token == 'return':
                self.compile_return()
        self._write_tag('statements', 'end')

    def compile_let(self):
        self._write_tag('letStatement', 'start')
        self._process('let')
        self._process() # varName
        
        # 陣列處理 let a[i] = ...
        if self.tokenizer.current_token == '[':
            self._process('[')
            self.compile_expression()
            self._process(']')
            
        self._process('=')
        self.compile_expression()
        self._process(';')
        self._write_tag('letStatement', 'end')

    def compile_if(self):
        self._write_tag('ifStatement', 'start')
        self._process('if')
        self._process('(')
        self.compile_expression()
        self._process(')')
        self._process('{')
        self.compile_statements()
        self._process('}')
        
        if self.tokenizer.current_token == 'else':
            self._process('else')
            self._process('{')
            self.compile_statements()
            self._process('}')
        self._write_tag('ifStatement', 'end')

    def compile_while(self):
        self._write_tag('whileStatement', 'start')
        self._process('while')
        self._process('(')
        self.compile_expression()
        self._process(')')
        self._process('{')
        self.compile_statements()
        self._process('}')
        self._write_tag('whileStatement', 'end')

    def compile_do(self):
        self._write_tag('doStatement', 'start')
        self._process('do')
        
        # SubroutineCall: 這裡稍微複雜，因為可能是 name() 或 Class.name()
        # 但我們可以簡單地連續 process，因為我們不需要區分語意，只需要結構
        self._process() # name (className or varName or funcName)
        if self.tokenizer.current_token == '.':
            self._process('.')
            self._process() # subroutineName
            
        self._process('(')
        self.compile_expression_list()
        self._process(')')
        self._process(';')
        self._write_tag('doStatement', 'end')

    def compile_return(self):
        self._write_tag('returnStatement', 'start')
        self._process('return')
        if self.tokenizer.current_token != ';':
            self.compile_expression()
        self._process(';')
        self._write_tag('returnStatement', 'end')

    # --- 表達式編譯方法 (Expressions) ---

    def compile_expression(self):
        self._write_tag('expression', 'start')
        self.compile_term()
        
        # (op term)*
        while self.tokenizer.current_token in OPS:
            self._process() # op
            self.compile_term()
            
        self._write_tag('expression', 'end')

    def compile_term(self):
        self._write_tag('term', 'start')
        
        token = self.tokenizer.current_token
        type = self.tokenizer.token_type
        
        # 1. Integer, String, Keyword
        if type in ['INT_CONST', 'STRING_CONST', 'KEYWORD']:
            self._process()
            
        # 2. VarName / Array / SubroutineCall
        elif type == 'IDENTIFIER':
            # 需要向前看 (Lookahead) 決定是單純變數、陣列還是函數呼叫
            next_token = self.tokenizer.peek()
            
            if next_token == '[': # Array: var[exp]
                self._process() # varName
                self._process('[')
                self.compile_expression()
                self._process(']')
            elif next_token == '(' or next_token == '.': # Subroutine Call
                self._process() # name
                if self.tokenizer.current_token == '.':
                    self._process('.')
                    self._process() # subName
                self._process('(')
                self.compile_expression_list()
                self._process(')')
            else: # Simple variable
                self._process()
                
        # 3. ( expression )
        elif token == '(':
            self._process('(')
            self.compile_expression()
            self._process(')')
            
        # 4. UnaryOp term (~term, -term)
        elif token in ['-', '~']:
            self._process() # unaryOp
            self.compile_term()
            
        self._write_tag('term', 'end')

    def compile_expression_list(self):
        self._write_tag('expressionList', 'start')
        if self.tokenizer.current_token != ')':
            self.compile_expression()
            while self.tokenizer.current_token == ',':
                self._process(',')
                self.compile_expression()
        self._write_tag('expressionList', 'end')


# ==========================================
# 4. 主程式 (Driver)
# ==========================================

def analyze_file(input_file):
    if not input_file.endswith('.jack'):
        return
    
    # 產生的檔案名稱將是 .xml (例如 Main.jack -> Main.xml)
    # 這裡要注意，第10章有兩個輸出：T.xml (Tokens) 和 .xml (Parse Tree)
    # 此程式只輸出 Parse Tree (因為這是主要的評分對象)
    
    output_file = input_file.replace('.jack', '.xml')
    print(f"Compiling {input_file} -> {output_file}")
    
    tokenizer = JackTokenizer(input_file)
    engine = CompilationEngine(tokenizer, output_file)
    engine.compile_class()
    engine.close()

def main():
    if len(sys.argv) != 2:
        print("Usage: python JackAnalyzer.py [file.jack|dir]")
        return
    
    path = sys.argv[1]
    
    if os.path.isdir(path):
        for filename in os.listdir(path):
            if filename.endswith(".jack"):
                analyze_file(os.path.join(path, filename))
    elif os.path.isfile(path):
        analyze_file(path)
    else:
        print("Invalid file or directory")

if __name__ == "__main__":
    main()
如何使用這段程式碼
保存：將上面的程式碼複製並存成 JackAnalyzer.py。
執行：打開終端機 (Terminal / Command Prompt)。
假設你要編譯 Square 目錄下的所有檔案：
code
Bash
python JackAnalyzer.py C:\Users\You\nand2tetris\projects\10\Square
或者只編譯單個檔案：
code
Bash
python JackAnalyzer.py Main.jack
實作重點解析 (為什麼這樣寫？)
Tokenizer 的正則表達式 (re.findall)：
手動寫一個逐字元讀取的 Tokenizer 很容易出錯（例如處理字串內的空格）。
我使用了 Python 的 re 模組。r'([a-zA-Z_]\w*)|(\d+)|("[^"\n]*")|([{}()\[\].,;+\-*/&|<>=~])' 這個 Regex 很暴力也很有效，它一次把所有的 Keyword, Identifier, Integer, String, Symbol 都抓出來分組。
CompilationEngine 的結構：
每個 compileXXX 方法都對應 Jack Grammar 的一條規則。
_process() 方法是一個通用的 Helper。它負責輸出當前的 XML，然後呼叫 tokenizer.advance() 移動到下一個 Token。這減少了大量的重複代碼。
XML 縮排：使用 indent_level 來控制。每當進入一個結構 (start) 就 +1，離開 (end) 就 -1。
Expression 的處理 (最難的部分)：
在 compile_expression 中，你可以看到 while self.tokenizer.current_token in OPS:。這處理了像 x + y * z 這樣連續運算的情況。
在 compile_term 中，我們需要 Lookahead (偷看下一個 Token)。因為當我們看到一個變數名 foo 時，我們不知道它是 foo (變數)、foo[i] (陣列) 還是 foo() (函數)。所以我在 JackTokenizer 裡加了一個 peek() 方法來解決這個歧義。
這份程式碼應該可以直接通過 Course Grader 的測試。如果遇到 MainT.xml (Token XML) 的比對問題，那部分是針對 Tokenizer 的單獨測試，這份程式碼直接生成了最終的 Syntax Tree XML，通常這已經足夠完成專案目標。
Model
ThinkingThoughts
Expand to view model thoughts

chevron_right
沒錯，你的猜測是對的。
Nand2Tetris 的課程資料夾（例如 projects/10/Square）裡面通常已經包含了作者提供的標準答案 .xml 檔（用來讓你比對用的）。原本的程式碼會直接覆蓋掉這些檔案，導致你看不到變化，甚至把標準答案蓋掉了。
為了避免這種情況，並讓你更容易檢查結果，我修改了程式碼。現在程式會在目標目錄下自動建立一個名為 output 的資料夾，並將生成的 XML 檔案都存進去。
修改後的 Python 程式碼
請將這段程式碼覆蓋原本的檔案。
code
Python
import sys
import os
import re

# ==========================================
# 1. 基礎定義 (關鍵字與符號)
# ==========================================

KEYWORDS = {
    'class', 'constructor', 'function', 'method', 'field', 'static', 'var',
    'int', 'char', 'boolean', 'void', 'true', 'false', 'null', 'this',
    'let', 'do', 'if', 'else', 'while', 'return'
}

SYMBOLS = {
    '{', '}', '(', ')', '[', ']', '.', ',', ';', '+', '-', '*', '/', '&',
    '|', '<', '>', '=', '~'
}

XML_MAP = {
    '<': '&lt;',
    '>': '&gt;',
    '"': '&quot;',
    '&': '&amp;'
}

OPS = {'+', '-', '*', '/', '&', '|', '<', '>', '='}

# ==========================================
# 2. JackTokenizer (分詞器)
# ==========================================

class JackTokenizer:
    def __init__(self, input_file):
        with open(input_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        self.tokens = self._tokenize(content)
        self.current_token_idx = 0
        self.current_token = ""
        self.token_type = ""

    def _tokenize(self, content):
        # 移除區塊註釋
        content = re.sub(r'/\*.*?\*/', ' ', content, flags=re.S)
        # 移除單行註釋
        content = re.sub(r'//.*', ' ', content)
        
        # Regex 提取 Token
        regex = r'([a-zA-Z_]\w*)|(\d+)|("[^"\n]*")|([{}()\[\].,;+\-*/&|<>=~])'
        matches = re.findall(regex, content)
        
        tokens = []
        for match in matches:
            for group in match:
                if group:
                    tokens.append(group)
        return tokens

    def has_more_tokens(self):
        return self.current_token_idx < len(self.tokens)

    def advance(self):
        if self.has_more_tokens():
            self.current_token = self.tokens[self.current_token_idx]
            self.current_token_idx += 1
            self._set_token_type()

    def _set_token_type(self):
        token = self.current_token
        if token in KEYWORDS:
            self.token_type = 'KEYWORD'
        elif token in SYMBOLS:
            self.token_type = 'SYMBOL'
        elif token.isdigit():
            self.token_type = 'INT_CONST'
        elif token.startswith('"'):
            self.token_type = 'STRING_CONST'
        else:
            self.token_type = 'IDENTIFIER'

    def string_val(self): return self.current_token[1:-1]

    def peek(self):
        if self.has_more_tokens():
            return self.tokens[self.current_token_idx]
        return ""

# ==========================================
# 3. CompilationEngine (語法分析引擎)
# ==========================================

class CompilationEngine:
    def __init__(self, tokenizer, output_path):
        # 確保父目錄存在
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        self.outfile = open(output_path, 'w', encoding='utf-8')
        self.tokenizer = tokenizer
        self.indent_level = 0
        
        if self.tokenizer.has_more_tokens():
            self.tokenizer.advance()

    def close(self):
        self.outfile.close()

    def _write_tag(self, tag, value=None, is_terminal=False):
        indent = '  ' * self.indent_level
        if is_terminal:
            val_escaped = XML_MAP.get(value, value)
            self.outfile.write(f"{indent}<{tag}> {val_escaped} </{tag}>\n")
        else:
            if value == 'start':
                self.outfile.write(f"{indent}<{tag}>\n")
                self.indent_level += 1
            elif value == 'end':
                self.indent_level -= 1
                indent = '  ' * self.indent_level
                self.outfile.write(f"{indent}</{tag}>\n")

    def _process(self, expected_token=None):
        curr_token = self.tokenizer.current_token
        curr_type = self.tokenizer.token_type
        
        tag_map = {
            'KEYWORD': 'keyword',
            'SYMBOL': 'symbol',
            'IDENTIFIER': 'identifier',
            'INT_CONST': 'integerConstant',
            'STRING_CONST': 'stringConstant'
        }
        
        tag = tag_map.get(curr_type)
        if curr_type == 'STRING_CONST':
            value = self.tokenizer.string_val()
        else:
            value = curr_token

        self._write_tag(tag, value, is_terminal=True)
        self.tokenizer.advance()

    # --- Structure ---
    def compile_class(self):
        self._write_tag('class', 'start')
        self._process('class')
        self._process()
        self._process('{')
        while self.tokenizer.current_token in ['static', 'field']:
            self.compile_class_var_dec()
        while self.tokenizer.current_token in ['constructor', 'function', 'method']:
            self.compile_subroutine()
        self._process('}')
        self._write_tag('class', 'end')

    def compile_class_var_dec(self):
        self._write_tag('classVarDec', 'start')
        self._process() 
        self._process()
        self._process()
        while self.tokenizer.current_token == ',':
            self._process(',')
            self._process()
        self._process(';')
        self._write_tag('classVarDec', 'end')

    def compile_subroutine(self):
        self._write_tag('subroutineDec', 'start')
        self._process()
        self._process()
        self._process()
        self._process('(')
        self.compile_parameter_list()
        self._process(')')
        self.compile_subroutine_body()
        self._write_tag('subroutineDec', 'end')

    def compile_parameter_list(self):
        self._write_tag('parameterList', 'start')
        if self.tokenizer.current_token != ')':
            self._process()
            self._process()
            while self.tokenizer.current_token == ',':
                self._process(',')
                self._process()
                self._process()
        self._write_tag('parameterList', 'end')

    def compile_subroutine_body(self):
        self._write_tag('subroutineBody', 'start')
        self._process('{')
        while self.tokenizer.current_token == 'var':
            self.compile_var_dec()
        self.compile_statements()
        self._process('}')
        self._write_tag('subroutineBody', 'end')

    def compile_var_dec(self):
        self._write_tag('varDec', 'start')
        self._process('var')
        self._process()
        self._process()
        while self.tokenizer.current_token == ',':
            self._process(',')
            self._process()
        self._process(';')
        self._write_tag('varDec', 'end')

    # --- Statements ---
    def compile_statements(self):
        self._write_tag('statements', 'start')
        while self.tokenizer.current_token in ['let', 'if', 'while', 'do', 'return']:
            if self.tokenizer.current_token == 'let': self.compile_let()
            elif self.tokenizer.current_token == 'if': self.compile_if()
            elif self.tokenizer.current_token == 'while': self.compile_while()
            elif self.tokenizer.current_token == 'do': self.compile_do()
            elif self.tokenizer.current_token == 'return': self.compile_return()
        self._write_tag('statements', 'end')

    def compile_let(self):
        self._write_tag('letStatement', 'start')
        self._process('let')
        self._process()
        if self.tokenizer.current_token == '[':
            self._process('[')
            self.compile_expression()
            self._process(']')
        self._process('=')
        self.compile_expression()
        self._process(';')
        self._write_tag('letStatement', 'end')

    def compile_if(self):
        self._write_tag('ifStatement', 'start')
        self._process('if')
        self._process('(')
        self.compile_expression()
        self._process(')')
        self._process('{')
        self.compile_statements()
        self._process('}')
        if self.tokenizer.current_token == 'else':
            self._process('else')
            self._process('{')
            self.compile_statements()
            self._process('}')
        self._write_tag('ifStatement', 'end')

    def compile_while(self):
        self._write_tag('whileStatement', 'start')
        self._process('while')
        self._process('(')
        self.compile_expression()
        self._process(')')
        self._process('{')
        self.compile_statements()
        self._process('}')
        self._write_tag('whileStatement', 'end')

    def compile_do(self):
        self._write_tag('doStatement', 'start')
        self._process('do')
        self._process()
        if self.tokenizer.current_token == '.':
            self._process('.')
            self._process()
        self._process('(')
        self.compile_expression_list()
        self._process(')')
        self._process(';')
        self._write_tag('doStatement', 'end')

    def compile_return(self):
        self._write_tag('returnStatement', 'start')
        self._process('return')
        if self.tokenizer.current_token != ';':
            self.compile_expression()
        self._process(';')
        self._write_tag('returnStatement', 'end')

    # --- Expressions ---
    def compile_expression(self):
        self._write_tag('expression', 'start')
        self.compile_term()
        while self.tokenizer.current_token in OPS:
            self._process()
            self.compile_term()
        self._write_tag('expression', 'end')

    def compile_term(self):
        self._write_tag('term', 'start')
        token = self.tokenizer.current_token
        type = self.tokenizer.token_type
        
        if type in ['INT_CONST', 'STRING_CONST', 'KEYWORD']:
            self._process()
        elif type == 'IDENTIFIER':
            next_token = self.tokenizer.peek()
            if next_token == '[':
                self._process()
                self._process('[')
                self.compile_expression()
                self._process(']')
            elif next_token == '(' or next_token == '.':
                self._process()
                if self.tokenizer.current_token == '.':
                    self._process('.')
                    self._process()
                self._process('(')
                self.compile_expression_list()
                self._process(')')
            else:
                self._process()
        elif token == '(':
            self._process('(')
            self.compile_expression()
            self._process(')')
        elif token in ['-', '~']:
            self._process()
            self.compile_term()
        self._write_tag('term', 'end')

    def compile_expression_list(self):
        self._write_tag('expressionList', 'start')
        if self.tokenizer.current_token != ')':
            self.compile_expression()
            while self.tokenizer.current_token == ',':
                self._process(',')
                self.compile_expression()
        self._write_tag('expressionList', 'end')

# ==========================================
# 4. 主程式 (修改後：新增 output 資料夾邏輯)
# ==========================================

def analyze_file(input_file, output_dir):
    """
    input_file: 原始 .jack 檔案的完整路徑
    output_dir: 輸出的資料夾路徑
    """
    if not input_file.endswith('.jack'):
        return
    
    # 取得原始檔名 (例如 Main.jack)
    base_name = os.path.basename(input_file)
    # 替換副檔名 (例如 Main.xml)
    xml_name = base_name.replace('.jack', '.xml')
    # 組合完整的輸出路徑
    output_path = os.path.join(output_dir, xml_name)
    
    print(f"Compiling: {base_name} -> output/{xml_name}")
    
    tokenizer = JackTokenizer(input_file)
    engine = CompilationEngine(tokenizer, output_path)
    engine.compile_class()
    engine.close()

def main():
    if len(sys.argv) != 2:
        print("Usage: python JackAnalyzer.py [file.jack|dir]")
        return
    
    path = sys.argv[1]
    
    # 定義輸出資料夾名稱
    OUTPUT_FOLDER_NAME = "output"

    if os.path.isdir(path):
        # 如果輸入是目錄，在該目錄下建立 output 資料夾
        output_dir = os.path.join(path, OUTPUT_FOLDER_NAME)
        # exist_ok=True 表示如果資料夾已存在也不會報錯
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"Processing directory: {path}")
        print(f"Output directory: {output_dir}\n")

        for filename in os.listdir(path):
            if filename.endswith(".jack"):
                full_input_path = os.path.join(path, filename)
                analyze_file(full_input_path, output_dir)
                
    elif os.path.isfile(path):
        # 如果輸入是檔案，在該檔案所在目錄下建立 output 資料夾
        dir_path = os.path.dirname(path)
        output_dir = os.path.join(dir_path, OUTPUT_FOLDER_NAME)
        os.makedirs(output_dir, exist_ok=True)
        
        analyze_file(path, output_dir)
    else:
        print("Invalid file or directory")

if __name__ == "__main__":
    main()
這次的改動點：
新增 output 資料夾：在 main 函數中，程式會偵測你輸入的路徑，並在該路徑下建立一個名為 output 的資料夾。
檔案路徑處理：analyze_file 現在接收 output_dir 參數，它會將生成的 XML 檔名與這個資料夾路徑結合，確保檔案寫入新資料夾中，而不是覆蓋原檔。
Log 顯示：執行時會顯示 Compiling: Main.jack -> output/Main.xml，讓你知道它正在寫入新位置。
執行方式不變：
code
Bash
python JackAnalyzer.py C:\Users\...\projects\10\Square
現在你去那個資料夾看，會多出一個 output 資料夾，你的 XML 都在裡面，原來的檔案會保持原樣。
Model
ThinkingThoughts
Expand to view model thoughts

chevron_right
Nand2Tetris 第十一章 (Chapter 11) 的主題是 「編譯器 II：程式碼生成 (Compiler II: Code Generation)」。
這是整個軟體架構中最關鍵的一步。在第 10 章，你已經將 Jack 原始碼轉換成了 XML 語法樹（Parser）；在第 11 章，你的目標是將這些語法結構轉換成 VM Code (虛擬機器碼)。
完成這一章後，你寫的 Jack 程式（例如貪食蛇遊戲）就能真正地在電腦上跑起來了！
以下是這一章的核心內容與實作重點：
核心目標
輸入：Jack 原始碼（或第 10 章的 Parse Tree）。
輸出：.vm 檔案（Stack-based 的虛擬機器指令）。
兩大核心挑戰
為了產生 VM Code，你需要解決兩個主要問題：資料怎麼存？ 與 指令怎麼轉？
1. 資料處理：符號表 (Symbol Table)
虛擬機器（VM）不懂變數的名字（如 salary, count, player），它只懂記憶體區段和索引（如 local 0, argument 1, this 2）。
你需要實作一個 Symbol Table 模組，用來記錄變數的資訊：
名稱 (Name): 變數的識別字（如 x）。
類型 (Type): int, char, boolean, 或類別名稱 Square。
種類 (Kind):
STATIC: 屬於類別的變數 (static 0)。
FIELD: 屬於物件的屬性 (this 0)。
ARG: 函數參數 (argument 0)。
VAR: 區域變數 (local 0)。
索引 (Index): 該種類下的第幾個變數（0, 1, 2...）。
實作細節：
你需要兩個層級的符號表：
Class-level: 記錄 static 和 field。
Subroutine-level: 記錄 argument 和 var。每當進入一個新的 function/method，這個表就要清空重置。
2. 邏輯轉換：堆疊運算 (Stack Operations)
Jack 是高階語言（使用 x = y + 5），VM 是堆疊語言（使用 push, pop, add）。
表達式 (Expressions): 將中序運算 (Infix) 轉為後序運算 (Postfix)。
Jack: x + 5
VM:
code
Vm
push local 0  // 假設 x 是 local 0
push constant 5
add
賦值 (Let):
Jack: let x = 5 + 2;
VM: 先算 5+2 (結果在 stack 頂端)，然後 pop x。
詳細實作內容
你將繼續使用第 10 章的架構（JackTokenizer 和 CompilationEngine），但要把「輸出 XML」的程式碼，換成「呼叫 VMWriter」的程式碼。
你需要新增兩個模組：
1. SymbolTable (符號表)
管理變數的作用域。
define(name, type, kind): 定義新變數，索引自動 +1。
varCount(kind): 回傳某種變數目前有幾個（用於計算 function MyFunc nVars）。
kindOf(name), typeOf(name), indexOf(name): 查詢變數屬性。
2. VMWriter (VM 寫入器)
簡單的封裝，負責輸出字串到 .vm 檔案。
writePush(segment, index)
writePop(segment, index)
writeArithmetic(command) (ADD, SUB, EQ, GT...)
writeLabel(label), writeGoto(label), writeIf(label)
writeCall(name, nArgs), writeFunction(name, nLocals)
特殊語法的處理邏輯 (最難的部分)
A. 流程控制 (If / While)
VM 沒有 if (...) { ... } 這種區塊，只有 goto 和 if-goto。你需要自動產生唯一的 Label。
While 迴圈轉換範例：
Jack: while (cond) { s1 }
VM 邏輯:
code
Vm
label L1
// 計算 cond
not
if-goto L2  // 如果 cond 為假，跳出迴圈
// 執行 s1
goto L1     // 跳回開頭
label L2
B. 函數呼叫 (Function vs Method)
Function (靜態函數): 直接呼叫 call ClassName.funcName nArgs。
Method (物件方法): 這是物件導向的關鍵。當你呼叫 obj.method(x) 時，其實隱藏了一個參數 this。
編譯器必須幫你把 obj 的記憶體位址先 push 到堆疊上。
然後呼叫方法（參數數量要 +1）。
在 Method 內部，第一行指令通常是 pop pointer 0（將剛傳進來的 this 設定為當前 context）。
C. 建構子 (Constructor)
Jack 的 constructor 需要處理記憶體配置。
編譯器要計算該類別有多少個 field 變數。
生成 push constant N (N = field 數量)。
呼叫 OS 的 call Memory.alloc 1 來索取記憶體。
將回傳的位址（物件指標）設定給 pointer 0 (this)。
D. 陣列處理 (Arrays)
Jack 的 arr[i] = y 涉及指標操作。
VM 邏輯：
Push arr (陣列起始位址)。
Push i (索引)。
add (計算出目標位址)。
利用 pointer 1 (that 指標) 來鎖定這個目標位址，進行讀寫。
實作步驟建議
擴充環境: 把第 10 章的程式碼複製一份。保留 Tokenizer，修改 CompilationEngine。
建立 SymbolTable: 先寫好這個類別，並確保能正確記錄變數。
實作 VMWriter: 寫好基礎的輸出功能。
修改 CompilationEngine:
變數宣告 (compileVarDec, compileClassVarDec): 不再輸出 XML，改為呼叫 SymbolTable.define()。
表達式 (compileExpression): 將變數名換成 SymbolTable.indexOf() 查到的 push 指令；將運算符號換成 add, sub 等指令。
語句 (compileLet, compileDo...): 實作上述的轉換邏輯。
總結
第 11 章是 Nand2Tetris 課程中最有成就感的一章。當你完成它，你就擁有了一個全功能的編譯器，可以編譯任何 Jack 程式（包括複雜的遊戲），並在虛擬機器上遊玩。這代表你完全打通了從「高階語言」到「底層指令」的橋樑。
Model
ThinkingThoughts
Expand to view model thoughts

chevron_right
這是一個完整的 Nand2Tetris 第 11 章：Jack Compiler (Code Generation) 的 Python 實作。
這個版本比第 10 章複雜許多，因為我們不再只是「翻譯結構」，而是要「維護狀態」（Symbol Table）並「計算邏輯」（VM Code）。
程式碼結構
JackTokenizer: (沿用) 負責切分 Token。
SymbolTable: (新增) 負責記錄變數的名稱、類型、種類(Static/Field/Arg/Var)與索引。
VMWriter: (新增) 負責輸出標準的 VM 指令。
CompilationEngine: (核心修改) 將 Parsing 邏輯改為 Code Generation 邏輯。
Main: (沿用) 處理檔案與資料夾路徑。
JackCompiler.py 完整程式碼
code
Python
import sys
import os
import re

# ==========================================
# 1. 基礎定義
# ==========================================

KEYWORDS = {
    'class', 'constructor', 'function', 'method', 'field', 'static', 'var',
    'int', 'char', 'boolean', 'void', 'true', 'false', 'null', 'this',
    'let', 'do', 'if', 'else', 'while', 'return'
}

SYMBOLS = {
    '{', '}', '(', ')', '[', ']', '.', ',', ';', '+', '-', '*', '/', '&',
    '|', '<', '>', '=', '~'
}

# 運算子對應到 VM 的指令或 OS 呼叫
OP_MAP = {
    '+': 'add', '-': 'sub', '*': 'call Math.multiply 2', '/': 'call Math.divide 2',
    '&': 'and', '|': 'or', '<': 'lt', '>': 'gt', '=': 'eq'
}

# Unary 運算子
UNARY_OP_MAP = {
    '-': 'neg', '~': 'not'
}

# Kind 到 Segment 的映射
KIND_TO_SEGMENT = {
    'STATIC': 'static',
    'FIELD': 'this',
    'ARG': 'argument',
    'VAR': 'local'
}

# ==========================================
# 2. JackTokenizer (與 Ch10 相同)
# ==========================================

class JackTokenizer:
    def __init__(self, input_file):
        with open(input_file, 'r', encoding='utf-8') as f:
            content = f.read()
        self.tokens = self._tokenize(content)
        self.current_token_idx = 0
        self.current_token = ""
        self.token_type = ""

    def _tokenize(self, content):
        content = re.sub(r'/\*.*?\*/', ' ', content, flags=re.S)
        content = re.sub(r'//.*', ' ', content)
        regex = r'([a-zA-Z_]\w*)|(\d+)|("[^"\n]*")|([{}()\[\].,;+\-*/&|<>=~])'
        matches = re.findall(regex, content)
        tokens = []
        for match in matches:
            for group in match:
                if group: tokens.append(group)
        return tokens

    def has_more_tokens(self):
        return self.current_token_idx < len(self.tokens)

    def advance(self):
        if self.has_more_tokens():
            self.current_token = self.tokens[self.current_token_idx]
            self.current_token_idx += 1
            self._set_token_type()

    def _set_token_type(self):
        token = self.current_token
        if token in KEYWORDS: self.token_type = 'KEYWORD'
        elif token in SYMBOLS: self.token_type = 'SYMBOL'
        elif token.isdigit(): self.token_type = 'INT_CONST'
        elif token.startswith('"'): self.token_type = 'STRING_CONST'
        else: self.token_type = 'IDENTIFIER'

    def string_val(self): return self.current_token[1:-1]
    def peek(self):
        if self.has_more_tokens(): return self.tokens[self.current_token_idx]
        return ""

# ==========================================
# 3. SymbolTable (符號表 - Ch11 新增)
# ==========================================

class SymbolTable:
    def __init__(self):
        self.class_scope = {}      # {name: (type, kind, index)}
        self.subroutine_scope = {} # {name: (type, kind, index)}
        self.counts = {'STATIC': 0, 'FIELD': 0, 'ARG': 0, 'VAR': 0}

    def start_subroutine(self):
        self.subroutine_scope = {}
        self.counts['ARG'] = 0
        self.counts['VAR'] = 0

    def define(self, name, type, kind):
        # kind: STATIC, FIELD, ARG, VAR
        index = self.counts[kind]
        record = (type, kind, index)
        
        if kind in ['STATIC', 'FIELD']:
            self.class_scope[name] = record
        else:
            self.subroutine_scope[name] = record
            
        self.counts[kind] += 1

    def var_count(self, kind):
        return self.counts[kind]

    def kind_of(self, name):
        if name in self.subroutine_scope: return self.subroutine_scope[name][1]
        if name in self.class_scope: return self.class_scope[name][1]
        return None

    def type_of(self, name):
        if name in self.subroutine_scope: return self.subroutine_scope[name][0]
        if name in self.class_scope: return self.class_scope[name][0]
        return None

    def index_of(self, name):
        if name in self.subroutine_scope: return self.subroutine_scope[name][2]
        if name in self.class_scope: return self.class_scope[name][2]
        return None

# ==========================================
# 4. VMWriter (VM 輸出器 - Ch11 新增)
# ==========================================

class VMWriter:
    def __init__(self, output_file):
        self.outfile = open(output_file, 'w', encoding='utf-8')

    def close(self):
        self.outfile.close()

    def write_push(self, segment, index):
        self.outfile.write(f"push {segment} {index}\n")

    def write_pop(self, segment, index):
        self.outfile.write(f"pop {segment} {index}\n")

    def write_arithmetic(self, command):
        self.outfile.write(f"{command}\n")

    def write_label(self, label):
        self.outfile.write(f"label {label}\n")

    def write_goto(self, label):
        self.outfile.write(f"goto {label}\n")

    def write_if(self, label):
        self.outfile.write(f"if-goto {label}\n")

    def write_call(self, name, n_args):
        self.outfile.write(f"call {name} {n_args}\n")

    def write_function(self, name, n_locals):
        self.outfile.write(f"function {name} {n_locals}\n")

    def write_return(self):
        self.outfile.write("return\n")

# ==========================================
# 5. CompilationEngine (核心邏輯 - Ch11 修改)
# ==========================================

class CompilationEngine:
    def __init__(self, tokenizer, output_path):
        self.tokenizer = tokenizer
        self.vm_writer = VMWriter(output_path)
        self.symbol_table = SymbolTable()
        self.class_name = ""
        self.label_counter = 0
        
        if self.tokenizer.has_more_tokens():
            self.tokenizer.advance()

    def close(self):
        self.vm_writer.close()

    def _eat(self, token=None):
        # 簡單的推進，如果有傳入 token 可以做檢查 (此處省略嚴格檢查)
        val = self.tokenizer.current_token
        self.tokenizer.advance()
        return val

    def _new_label(self):
        self.label_counter += 1
        return f"L{self.label_counter}"

    # --- Structure ---

    def compile_class(self):
        self._eat('class')
        self.class_name = self._eat() # ClassName
        self._eat('{')
        
        while self.tokenizer.current_token in ['static', 'field']:
            self.compile_class_var_dec()
            
        while self.tokenizer.current_token in ['constructor', 'function', 'method']:
            self.compile_subroutine()
            
        self._eat('}')

    def compile_class_var_dec(self):
        kind_str = self._eat() # static / field
        kind = kind_str.upper()
        type = self._eat()     # int / char / ...
        name = self._eat()     # varName
        self.symbol_table.define(name, type, kind)
        
        while self.tokenizer.current_token == ',':
            self._eat(',')
            name = self._eat()
            self.symbol_table.define(name, type, kind)
        self._eat(';')

    def compile_subroutine(self):
        self.symbol_table.start_subroutine()
        
        sub_type = self._eat() # constructor / function / method
        return_type = self._eat()
        sub_name = self._eat()
        
        # Method 的第一個隱藏參數是 this
        if sub_type == 'method':
            self.symbol_table.define('this', self.class_name, 'ARG')
            
        self._eat('(')
        self.compile_parameter_list()
        self._eat(')')
        
        # Subroutine Body
        self._eat('{')
        while self.tokenizer.current_token == 'var':
            self.compile_var_dec()
            
        # 寫入 function 定義: function Class.Name nLocals
        full_name = f"{self.class_name}.{sub_name}"
        n_locals = self.symbol_table.var_count('VAR')
        self.vm_writer.write_function(full_name, n_locals)
        
        # 特殊處理: Constructor 需要 alloc, Method 需要設定 this
        if sub_type == 'constructor':
            field_count = self.symbol_table.var_count('FIELD')
            self.vm_writer.write_push('constant', field_count)
            self.vm_writer.write_call('Memory.alloc', 1)
            self.vm_writer.write_pop('pointer', 0) # this = alloc()
        elif sub_type == 'method':
            self.vm_writer.write_push('argument', 0)
            self.vm_writer.write_pop('pointer', 0) # this = arg0
            
        self.compile_statements()
        self._eat('}')

    def compile_parameter_list(self):
        if self.tokenizer.current_token != ')':
            type = self._eat()
            name = self._eat()
            self.symbol_table.define(name, type, 'ARG')
            while self.tokenizer.current_token == ',':
                self._eat(',')
                type = self._eat()
                name = self._eat()
                self.symbol_table.define(name, type, 'ARG')

    def compile_var_dec(self):
        self._eat('var')
        type = self._eat()
        name = self._eat()
        self.symbol_table.define(name, type, 'VAR')
        while self.tokenizer.current_token == ',':
            self._eat(',')
            name = self._eat()
            self.symbol_table.define(name, type, 'VAR')
        self._eat(';')

    # --- Statements ---

    def compile_statements(self):
        while self.tokenizer.current_token in ['let', 'if', 'while', 'do', 'return']:
            if self.tokenizer.current_token == 'let': self.compile_let()
            elif self.tokenizer.current_token == 'if': self.compile_if()
            elif self.tokenizer.current_token == 'while': self.compile_while()
            elif self.tokenizer.current_token == 'do': self.compile_do()
            elif self.tokenizer.current_token == 'return': self.compile_return()

    def compile_let(self):
        self._eat('let')
        var_name = self._eat()
        is_array = False
        
        # 處理陣列賦值: let a[i] = x
        if self.tokenizer.current_token == '[':
            is_array = True
            # Push array base address
            kind = self.symbol_table.kind_of(var_name)
            index = self.symbol_table.index_of(var_name)
            self.vm_writer.write_push(KIND_TO_SEGMENT[kind], index)
            
            self._eat('[')
            self.compile_expression() # 計算 index
            self._eat(']')
            
            self.vm_writer.write_arithmetic('add') # base + index
            
        self._eat('=')
        self.compile_expression() # 計算 RHS
        self._eat(';')
        
        if is_array:
            self.vm_writer.write_pop('temp', 0)    # 暫存 RHS 結果
            self.vm_writer.write_pop('pointer', 1) # 設定 that 到目標位址
            self.vm_writer.write_push('temp', 0)   # 取回 RHS
            self.vm_writer.write_pop('that', 0)    # 寫入
        else:
            # 一般變數賦值
            kind = self.symbol_table.kind_of(var_name)
            index = self.symbol_table.index_of(var_name)
            if kind:
                self.vm_writer.write_pop(KIND_TO_SEGMENT[kind], index)

    def compile_if(self):
        l1 = self._new_label()
        l2 = self._new_label()
        
        self._eat('if')
        self._eat('(')
        self.compile_expression()
        self._eat(')')
        
        self.vm_writer.write_arithmetic('not')
        self.vm_writer.write_if(l1) # 如果條件不成立，跳到 L1 (else/end)
        
        self._eat('{')
        self.compile_statements()
        self._eat('}')
        
        self.vm_writer.write_goto(l2) # 跳過 else 區塊
        
        self.vm_writer.write_label(l1)
        if self.tokenizer.current_token == 'else':
            self._eat('else')
            self._eat('{')
            self.compile_statements()
            self._eat('}')
            
        self.vm_writer.write_label(l2)

    def compile_while(self):
        l1 = self._new_label()
        l2 = self._new_label()
        
        self.vm_writer.write_label(l1)
        
        self._eat('while')
        self._eat('(')
        self.compile_expression()
        self._eat(')')
        
        self.vm_writer.write_arithmetic('not')
        self.vm_writer.write_if(l2) # 條件假，跳出迴圈
        
        self._eat('{')
        self.compile_statements()
        self._eat('}')
        
        self.vm_writer.write_goto(l1)
        self.vm_writer.write_label(l2)

    def compile_do(self):
        self._eat('do')
        # Do 語句其實就是一個表達式呼叫，但我們會丟棄回傳值
        self.compile_term() # 這會處理函數呼叫邏輯
        self._eat(';')
        self.vm_writer.write_pop('temp', 0) # 丟棄 void 函數預設回傳的 0

    def compile_return(self):
        self._eat('return')
        if self.tokenizer.current_token != ';':
            self.compile_expression()
        else:
            self.vm_writer.write_push('constant', 0) # void return 0
        self._eat(';')
        self.vm_writer.write_return()

    # --- Expressions ---

    def compile_expression(self):
        self.compile_term()
        while self.tokenizer.current_token in OP_MAP:
            op = self._eat()
            self.compile_term()
            # 輸出運算指令 (Postfix)
            if op in OP_MAP:
                cmd = OP_MAP[op]
                if cmd.startswith('call'):
                    # 處理 Math.multiply / Math.divide
                    parts = cmd.split()
                    self.vm_writer.write_call(parts[1], int(parts[2]))
                else:
                    self.vm_writer.write_arithmetic(cmd)

    def compile_term(self):
        token = self.tokenizer.current_token
        type = self.tokenizer.token_type
        
        if type == 'INT_CONST':
            val = self._eat()
            self.vm_writer.write_push('constant', val)
            
        elif type == 'STRING_CONST':
            s = self.tokenizer.string_val()
            self._eat()
            self.vm_writer.write_push('constant', len(s))
            self.vm_writer.write_call('String.new', 1)
            for char in s:
                self.vm_writer.write_push('constant', ord(char))
                self.vm_writer.write_call('String.appendChar', 2)
                
        elif type == 'KEYWORD':
            val = self._eat()
            if val == 'true':
                self.vm_writer.write_push('constant', 1) # 1
                self.vm_writer.write_arithmetic('neg')   # -1
            elif val in ['false', 'null']:
                self.vm_writer.write_push('constant', 0)
            elif val == 'this':
                self.vm_writer.write_push('pointer', 0)
                
        elif type == 'IDENTIFIER':
            # 需要 Lookahead 來判斷是 變數 / 陣列 / 函數呼叫
            name = self._eat()
            next_token = self.tokenizer.current_token
            
            if next_token == '[': # Array: a[i]
                self._eat('[')
                
                # Push array base
                kind = self.symbol_table.kind_of(name)
                idx = self.symbol_table.index_of(name)
                self.vm_writer.write_push(KIND_TO_SEGMENT[kind], idx)
                
                self.compile_expression() # index
                self._eat(']')
                
                self.vm_writer.write_arithmetic('add')
                self.vm_writer.write_pop('pointer', 1) # that = arr + i
                self.vm_writer.write_push('that', 0)
                
            elif next_token == '(' or next_token == '.': # Function Call
                self._compile_subroutine_call(name)
                
            else: # Simple Variable
                kind = self.symbol_table.kind_of(name)
                idx = self.symbol_table.index_of(name)
                if kind:
                    self.vm_writer.write_push(KIND_TO_SEGMENT[kind], idx)
                    
        elif token == '(':
            self._eat('(')
            self.compile_expression()
            self._eat(')')
            
        elif token in UNARY_OP_MAP:
            op = self._eat()
            self.compile_term()
            self.vm_writer.write_arithmetic(UNARY_OP_MAP[op])

    def _compile_subroutine_call(self, first_name):
        # 這裡處理 foo() 或 Class.foo() 或 var.method()
        n_args = 0
        full_func_name = ""
        
        if self.tokenizer.current_token == '.':
            self._eat('.')
            sub_name = self._eat()
            
            # 檢查 first_name 是 類別名 還是 變數名
            kind = self.symbol_table.kind_of(first_name)
            if kind: 
                # 是變數 (e.g., ball.move()) -> Method Call
                # Push 'this' (the object)
                idx = self.symbol_table.index_of(first_name)
                self.vm_writer.write_push(KIND_TO_SEGMENT[kind], idx)
                
                # 取得變數的型別 (Class Name)
                class_type = self.symbol_table.type_of(first_name)
                full_func_name = f"{class_type}.{sub_name}"
                n_args = 1 # 已經 push 了一個 this
            else:
                # 是類別 (e.g., Math.abs()) -> Function Call
                full_func_name = f"{first_name}.{sub_name}"
        else:
            # 隱式 Method Call (e.g., draw()) -> this.draw()
            self.vm_writer.write_push('pointer', 0) # push this
            full_func_name = f"{self.class_name}.{first_name}"
            n_args = 1
            
        self._eat('(')
        n_args += self.compile_expression_list()
        self._eat(')')
        
        self.vm_writer.write_call(full_func_name, n_args)

    def compile_expression_list(self):
        count = 0
        if self.tokenizer.current_token != ')':
            self.compile_expression()
            count += 1
            while self.tokenizer.current_token == ',':
                self._eat(',')
                self.compile_expression()
                count += 1
        return count

# ==========================================
# 6. 主程式
# ==========================================

def analyze_file(input_file, output_dir):
    if not input_file.endswith('.jack'): return
    
    base_name = os.path.basename(input_file)
    vm_name = base_name.replace('.jack', '.vm')
    output_path = os.path.join(output_dir, vm_name)
    
    print(f"Compiling: {base_name} -> output/{vm_name}")
    
    tokenizer = JackTokenizer(input_file)
    engine = CompilationEngine(tokenizer, output_path)
    engine.compile_class()
    engine.close()

def main():
    if len(sys.argv) != 2:
        print("Usage: python JackCompiler.py [file.jack|dir]")
        return
    
    path = sys.argv[1]
    OUTPUT_FOLDER_NAME = "output"

    if os.path.isdir(path):
        output_dir = os.path.join(path, OUTPUT_FOLDER_NAME)
        os.makedirs(output_dir, exist_ok=True)
        print(f"Processing directory: {path}")
        print(f"Output directory: {output_dir}\n")

        for filename in os.listdir(path):
            if filename.endswith(".jack"):
                analyze_file(os.path.join(path, filename), output_dir)
                
    elif os.path.isfile(path):
        dir_path = os.path.dirname(path)
        output_dir = os.path.join(dir_path, OUTPUT_FOLDER_NAME)
        os.makedirs(output_dir, exist_ok=True)
        analyze_file(path, output_dir)
    else:
        print("Invalid file or directory")

if __name__ == "__main__":
    main()
如何使用
將程式碼存為 JackCompiler.py。
在終端機執行：
code
Bash
python JackCompiler.py <你的 Jack 檔案或目錄>
例如：
code
Bash
python JackCompiler.py C:\nand2tetris\projects\11\Seven
它會在目標目錄下產生一個 output 資料夾，裡面包含生成的 .vm 檔案。
程式實作亮點 (Ch 11 的關鍵變更)
SymbolTable 的引入：
在 CompilationEngine 裡，我現在會呼叫 symbol_table.define() 來記錄變數。
在 compile_term (處理變數時)，我使用 symbol_table.kind_of(name) 和 index_of(name) 來決定要產生 push local 0 還是 push static 5。
方法呼叫 (Method Call) 的處理：
在 _compile_subroutine_call 中，我加入了判斷邏輯。
如果是 obj.method()，編譯器會自動先產生 push obj (將物件參考壓入堆疊)，這就是物件導向中隱藏的 this 參數。
建構子 (Constructor) 的記憶體分配：
在 compile_subroutine 裡，如果發現是 constructor，會先計算 Field 變數的數量，然後產生 call Memory.alloc，最後將回傳的位址設給 pointer 0。
字串常數處理：
Jack 的 VM 不支援直接 push 字串。所以當讀到 "Hello" 時，我生成了建立 String 物件並逐字 appendChar 的 VM 代碼。
你可以拿這個編譯器去跑 Pong 或 ComplexArrays 等大型專案，產生的 .vm 檔丟進 VM Emulator 應該能完美執行！
Model
ThinkingThoughts
Expand to view model thoughts

chevron_right
User
那為什麼兩章資料夾內容一樣
Model
ThinkingThoughts
Expand to view model thoughts

chevron_right
這是一個非常敏銳的觀察！這常常讓初學者感到困惑。
簡單來說：雖然檔案看起來一樣（都是 .jack 檔），但它們的「用途」完全相反。
這就像是 「考卷」 和 「標準答案」 的關係：
1. 角色互換：從「作者」變成「測試者」
在第九章 (projects/09)：
這些 .jack 檔案是你的 「作品」。
你打開這些檔案是為了 編輯它們（例如寫 Square 遊戲）。
你的目標是：寫出沒有 Bug 的 Jack 程式碼。
在第十一章 (projects/11)：
這些 .jack 檔案變成了你的 「測試題目」 (Test Cases)。
你 絕對不應該修改 這些檔案。
你的目標是：用你寫的 Python 編譯器去讀取這些檔案，看看能不能吐出正確的 .vm 檔。
2. 為什麼要用同一套檔案？
Nand2Tetris 故意使用相同的經典範例（如 Square、Average、Pong）貫穿這幾章，原因有二：
控制變因 (Control Variable)：
因為你已經在第 9 章玩過 Square 遊戲了，你知道它原本應該長怎樣、跑起來要是什麼樣子。
當你在第 11 章寫編譯器時，如果編譯出來的 Square 跑起來怪怪的（例如方塊只會往右跑，不會往左跑），你馬上就知道是你的編譯器在翻譯「減法」或「按鍵偵測」時出錯了。
難度分級 (Benchmarks)：
第 11 章的資料夾通常包含以下幾個等級，用來測試你編譯器的強壯程度：
Seven: 最簡單，只測試 1 + (2 * 3) 這種運算，沒有變數，沒有類別。
ConvertToBin: 測試記憶體操作 (Peek/Poke)。
Square: 測試完整的物件導向、建構子、方法呼叫。
Pong: 最終大魔王。這是完整的乒乓球遊戲，程式碼很大且複雜。如果你的編譯器能成功編譯 Pong，且遊戲能順暢執行，你的編譯器就是完美的。
3. 資料夾內容的細微差異
雖然都有 .jack 檔，但如果你仔細看，其實還有一點點不同：
Projects/09：通常只有原始碼，或者只有少量的範例。重點是讓你發揮創意。
Projects/11：除了 .jack 檔，通常還會附上 官方提供的標準 .vm 檔 (或是對比用的 XML)。
這是讓你用 TextComparer 工具來比對用的。
也就是說：你的 Python 程式產生的 VM 檔，內容必須跟官方給的 VM 檔一模一樣。
總結
第九章的 Square 是讓你 玩的。
第十一章的 Square 是拿來 給你的編譯器練拳的沙包。
所以，在做第 11 章時，請把那些 .jack 檔案當作神聖不可侵犯的「輸入資料」，只讀取，不修改。